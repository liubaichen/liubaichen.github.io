<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>深度学习(2)——神经网络 | Welcome to kotobanoniwa</title><meta name="author" content="Liu baichen"><meta name="copyright" content="Liu baichen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参考书目：深度学习入门(1)·斋藤康毅 激活函数 激活函数的作用就是对加权输入信号和偏置的总和进行函数处理，使之变为输出信号。单层感知机（朴素感知机）的激活函数为阶跃函数，如果将其替换为其他的激活函数，就会进化为神经网络。例如比较常用是sigmoid函数和ReLU函数： h(x)&#x3D;11+exp(−x)h(x)&#x3D;\frac{1}{1+exp(-x)}  h(x)&#x3D;1+exp(−x)1​ f(x)&#x3D;{">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习(2)——神经网络">
<meta property="og:url" content="https://liubaichen.github.io/posts/35526/index.html">
<meta property="og:site_name" content="Welcome to kotobanoniwa">
<meta property="og:description" content="参考书目：深度学习入门(1)·斋藤康毅 激活函数 激活函数的作用就是对加权输入信号和偏置的总和进行函数处理，使之变为输出信号。单层感知机（朴素感知机）的激活函数为阶跃函数，如果将其替换为其他的激活函数，就会进化为神经网络。例如比较常用是sigmoid函数和ReLU函数： h(x)&#x3D;11+exp(−x)h(x)&#x3D;\frac{1}{1+exp(-x)}  h(x)&#x3D;1+exp(−x)1​ f(x)&#x3D;{">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp12.webp">
<meta property="article:published_time" content="2023-03-30T01:23:20.000Z">
<meta property="article:modified_time" content="2023-04-08T08:21:08.054Z">
<meta property="article:author" content="Liu baichen">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp12.webp"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="https://liubaichen.github.io/posts/35526/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="G-F8JFWLJ64K"/><meta name="baidu-site-verification"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?c0ba518e647522d0484a4fd9f90b8f8d";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-F8JFWLJ64K"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-F8JFWLJ64K');
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习(2)——神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-08 16:21:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Welcome to kotobanoniwa" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/face.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp12.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="Welcome to kotobanoniwa"><img class="site-icon" src="/img/website.png"/><span class="site-name">Welcome to kotobanoniwa</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习(2)——神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-30T01:23:20.000Z" title="发表于 2023-03-30 09:23:20">2023-03-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-04-08T08:21:08.054Z" title="更新于 2023-04-08 16:21:08">2023-04-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/">python</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>参考书目：深度学习入门(1)·斋藤康毅</p>
<h1>激活函数</h1>
<p>激活函数的作用就是对加权输入信号和偏置的总和进行函数处理，使之变为输出信号。单层感知机（朴素感知机）的激活函数为阶跃函数，如果将其替换为其他的激活函数，就会进化为神经网络。例如比较常用是sigmoid函数和ReLU函数：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">h(x)=\frac{1}{1+exp(-x)} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2574em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>x&gt;0</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>x&lt;=0</mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex"> f(x)=
\begin{cases}
x&amp; \text{x&gt;0}\\
0&amp; \text{x&lt;=0}
\end{cases} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">x&gt;0</span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">x&lt;=0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>阶跃函数、sigmoid函数和ReLU函数的python实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">step_function</span>(<span class="params">a</span>):</span><br><span class="line">    <span class="keyword">return</span> np.array(a&gt;<span class="number">0</span>,dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">b</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span>+np.exp(b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">c</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>,c)</span><br></pre></td></tr></table></figure>
<p>输出层的激活函数与隐藏层不同，包括恒等函数和softmax函数。恒等函数一般用于回归问题，softmax函数一般用于分类问题。softmax函数的输出值介于0-1之间，可以看成概率，其大小关系与输出层的输入信号一致。因此学习过程使用softmax函数，但推理过程无需使用。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo>∑</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">y_k = \frac{exp(a_k)}{\sum exp(a_i)}  
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">d</span>):</span><br><span class="line">    d_max = np.<span class="built_in">max</span>(d)</span><br><span class="line">    <span class="keyword">return</span> np.exp(d-d_max) / np.<span class="built_in">sum</span>(np.exp(d-d_max))</span><br><span class="line"><span class="comment">#! 这里考虑了输入信号过大可能导致指数计算时出现错误</span></span><br></pre></td></tr></table></figure>
<h1>学习</h1>
<h2 id="loss-function">loss function</h2>
<p>神经网络的学习完全基于原始数据，而普通的机器学习（如支持向量机）则依赖于人工对原始数据的特征化处理，因此神经网络（深度学习）不存在人为介入</p>
<p>学习的指标统称为损失函数（loss function），常用的包括均方误差和交叉熵误差</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>×</mo><munder><mo>∑</mo><mi>k</mi></munder><mo stretchy="false">(</mo><msub><mi>y</mi><mi>k</mi></msub><mo>−</mo><msub><mi>t</mi><mi>k</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">E=\frac{1}{2}\times \sum_{k}(y_k-t_k)^2 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>k</mi></munder><msub><mi>t</mi><mi>k</mi></msub><mi>l</mi><mi>n</mi><mtext> </mtext><msub><mi>y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">E=-\sum_k t_kln\,y_k 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line">y = numpy.array([<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.6</span>,<span class="number">0.1</span>,<span class="number">0</span>])</span><br><span class="line">t = numpy.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#? 均方误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_error</span>(<span class="params">y,t</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>*numpy.<span class="built_in">sum</span>((y-t)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#? 交叉熵误差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y,t</span>):</span><br><span class="line">    delta = <span class="number">10e-7</span></span><br><span class="line">    <span class="keyword">return</span> -numpy.<span class="built_in">sum</span>(t*numpy.log(y+delta))</span><br><span class="line"></span><br><span class="line">mean_squared_error(y,t)</span><br><span class="line">cross_entropy_error(y,t)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>以上公式针对单个训练数据，k为数据的维数，y为训练结果，t为训练数据<br>
理论上需要对所有训练数据的损失函数求平均取平均损失函数，但考虑到数据量过大，选取一部分mini-batch进行多次训练</p>
</blockquote>
<p><strong>为什么设置损失函数，而不是以accuracy作为指标？</strong></p>
<p>核心问题：accuracy是一个离散值，当微调参数时，accuracy不一定发生变化（即accuracy对参数的导数值为0），无法指导我们修改参数</p>
<h2 id="学习步骤">学习步骤</h2>
<ol>
<li>学习数据和测试数据获取</li>
<li>构建神经网络</li>
<li>对每一个mini-batch计算梯度，并更新参数</li>
<li>在学习过程中记录accuracy，观察是否出现过拟合</li>
</ol>
<h1>随机梯度下降法</h1>
<p><strong>梯度指向函数下降最快的方向，而不一定是最小值方向</strong>。因此需要不断地在新点位计算梯度，并沿着新梯度前进。在神经网络中，使用的是损失函数对参数的梯度：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>=</mo><msub><mi>w</mi><mi>n</mi></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>f</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">w_n = w_n - \eta\frac{\partial f}{\partial x_n}  
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.2074em;vertical-align:-0.836em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>其中：w为参数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span> 为<strong>学习率（learning rate）</strong>，f为损失函数。学习率是也是神经网络的参数，但与权重和偏置不同，学习率需要人为指定，因此称为超参数。stable diffusion中的lr就是学习率。梯度获取最简单的方法是数值微分中的中心差分思想，但是真的慢！下一节的误差反向传播法的速度能够快很多。</p>
<h1>误差反向传播法</h1>
<p>误差反向传播法是一种快速的梯度获取方法</p>
<h2 id="理论基础">理论基础</h2>
<p>误差反向传播法基于偏导数中的链式法则</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>X</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>Y</mi></mrow></mfrac><mo>⋅</mo><msup><mi>W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial X}= \frac{\partial L}{\partial Y}\cdot W^T  
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac><mo>=</mo><msup><mi>X</mi><mi>T</mi></msup><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>Y</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial W}= X^T \cdot \frac{\partial L}{\partial Y} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>偏置的偏导数有些特殊：L对Y的偏导数是一个N行矩阵，而偏置的行数应该是1，因此需要按列求和(axis=0)</p>
<p><strong>计算要点</strong></p>
<ol>
<li>分支(repeat)节点：正向传播时有n个分支流出，则反向传播时需要对n个输入信号求和</li>
<li>并入(sum)节点：正向有并入求和过程，反向无需分离，相当于分支节点的正过程</li>
</ol>
<blockquote>
<p>应用：softmax_loss层推导<br>
上述内容来自《深度学习进阶：自然语言处理》的第一章，但是非常有用</p>
</blockquote>
<h2 id="层构建">层构建</h2>
<p>基于面向对象思想构建神经网络的层结构，包括激活函数、加权层（affine）和输出层（激活函数+loss）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#* 乘法层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mulayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.y = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x,y</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        out = x*y</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self,dout</span>):</span><br><span class="line">        dx = dout*self.y</span><br><span class="line">        dy = dout*self.x</span><br><span class="line">        <span class="keyword">return</span> dx,dy</span><br><span class="line"></span><br><span class="line">apple = mulayer()</span><br><span class="line">tax = mulayer()</span><br><span class="line">a1 = apple.forward(<span class="number">100.0</span>,<span class="number">2.</span>)</span><br><span class="line">a2 = tax.forward(a1,<span class="number">1.1</span>)</span><br><span class="line"><span class="built_in">print</span>(a2)</span><br><span class="line">dout = <span class="number">1</span></span><br><span class="line">b1,b2 = tax.backward(dout)</span><br><span class="line">b3,b4 = apple.backward(b1)</span><br><span class="line"><span class="built_in">print</span>(b1,b3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#* 加法层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">prolayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x,y</span>):</span><br><span class="line">        <span class="keyword">return</span> x+y</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self,dout</span>):</span><br><span class="line">        <span class="keyword">return</span> dout,dout</span><br><span class="line"></span><br><span class="line"><span class="comment">#* relu层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">relulayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,a</span>):</span><br><span class="line">        <span class="keyword">return</span> np.maximum(a,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self,dout</span>):</span><br><span class="line">        <span class="keyword">return</span> np.where(dout&gt;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">a = relulayer()</span><br><span class="line">a.forward(np.arange(-<span class="number">4</span>,<span class="number">5</span>).reshape([<span class="number">3</span>,<span class="number">3</span>]))</span><br><span class="line">a.backward(np.arange(-<span class="number">4</span>,<span class="number">5</span>).reshape([<span class="number">3</span>,<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#* sigmoid层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">sigmoidlayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.y = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        self.y = <span class="number">1</span> / (<span class="number">1</span>+np.exp(-x))</span><br><span class="line">        <span class="keyword">return</span> self.y</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self,dout</span>):</span><br><span class="line">        <span class="keyword">return</span> dout*self.y*(<span class="number">1.0</span>-self.y) <span class="comment">#type:ignore</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#! affine层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">affine</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,w,b</span>):</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.w = w</span><br><span class="line">        self.b = b</span><br><span class="line">        self.dw = <span class="literal">None</span></span><br><span class="line">        self.db = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        <span class="keyword">return</span> np.dot(self.x,self.w)+self.b</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self,dout</span>):</span><br><span class="line">        self.dw = np.dot(self.x.T,dout)<span class="comment">#type:ignore</span></span><br><span class="line">        self.db = np.<span class="built_in">sum</span>(dout,axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> np.dot(dout,self.w.T)</span><br><span class="line"></span><br><span class="line"><span class="comment">#! softmax_losslayer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">softmax_losslayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.loss = <span class="literal">None</span></span><br><span class="line">        self.y = <span class="literal">None</span></span><br><span class="line">        self.t = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x,t</span>):</span><br><span class="line">        self.y = softmax(x)</span><br><span class="line">        self.t = t</span><br><span class="line">        self.loss = cross_entropy_error(self.y,self.t)</span><br><span class="line">        <span class="keyword">return</span> self.loss</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self,dout</span>):</span><br><span class="line">        batch_size = self.t.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> (self.y-self.t) / batch_size</span><br></pre></td></tr></table></figure>
<h2 id="神经网络构建">神经网络构建</h2>
<p>有了上述的层结构，就可以构建神经网络了。由于需要进行正向和反向操作，选择有序数据类型是必要的。python3.7后dict是有序数据，与ordereddict的用法比较接近，但是考虑到兼容性，仍然使用ordereddict</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">a = OrderedDict()</span><br><span class="line">a[<span class="string">&#x27;one&#x27;</span>] = <span class="number">1</span></span><br><span class="line">a[<span class="string">&#x27;two&#x27;</span>] = <span class="number">2</span></span><br><span class="line">a[<span class="string">&#x27;three&#x27;</span>] = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> <span class="built_in">reversed</span>(a.values()):</span><br><span class="line">    <span class="built_in">print</span>(value)</span><br><span class="line">    </span><br><span class="line">a = &#123;&#125;</span><br><span class="line">a[<span class="string">&#x27;one&#x27;</span>] = <span class="number">1</span></span><br><span class="line">a[<span class="string">&#x27;two&#x27;</span>] = <span class="number">2</span></span><br><span class="line">a[<span class="string">&#x27;three&#x27;</span>] = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> <span class="built_in">reversed</span>(a.values()):</span><br><span class="line">    <span class="built_in">print</span>(value)</span><br><span class="line"></span><br><span class="line"><span class="comment">#以上代码在3.8.6中均可运行</span></span><br></pre></td></tr></table></figure>
<p>神经网络构建包括层组装和梯度获取两部分，前者使用上一小节建立的层结构和ordereddict构建，后者将各affine层的梯度汇总，然后更新参数。</p>
<h1>参数更新</h1>
<p>参数更新的核心数据是损失函数对参数的梯度，前面的两节中，都是基于随机梯度下降法（SGD），而如何更好地使用梯度就是本节的问题。</p>
<h2 id="SGD">SGD</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SGD</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,lr=<span class="number">0.01</span></span>):</span><br><span class="line">        self.lr = lr</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self,params,grads</span>):</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> params.keys():</span><br><span class="line">            params[key] -= self.lr*grads[key]</span><br></pre></td></tr></table></figure>
<p>SGD的问题在<strong>梯度不一定指向最小值方向</strong></p>
<h2 id="momentum">momentum</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! momentum</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">momentum</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,a=<span class="number">0.9</span>,lr = <span class="number">0.1</span></span>):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.v = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self,params,grads</span>):</span><br><span class="line">        <span class="keyword">if</span> self.v == <span class="literal">None</span>:</span><br><span class="line">            self.v = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> key,value <span class="keyword">in</span> params.items():</span><br><span class="line">                self.v[key] = np.zeros_like(value)</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> params.keys():</span><br><span class="line">            self.v[key] = self.v[key]*self.a-self.lr*grads[key]</span><br><span class="line">            params[key] += self.v[key]</span><br></pre></td></tr></table></figure>
<p>momentum记录了之前所有的梯度，因此可以抵消震荡导致的低效率。</p>
<h2 id="adagrad">adagrad</h2>
<p>adagrad比momentum更进一步，使用梯度参数改变学习率。具体来说，梯度越大的参数，学习率将减小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! adagrad</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">adagrad</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,lr = <span class="number">0.1</span></span>):</span><br><span class="line">        self.h = <span class="literal">None</span></span><br><span class="line">        self.lr = lr</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self,params,grads</span>):</span><br><span class="line">        <span class="keyword">if</span> self.h == <span class="literal">None</span>:</span><br><span class="line">            self.h = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> key,value <span class="keyword">in</span> params.items():</span><br><span class="line">                self.h[key] = np.zeros_like(value)</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> params.keys():</span><br><span class="line">            self.h[key] += grads[key]*grads[key]</span><br><span class="line">            params[key] -= self.lr*grads[key]/(np.sqrt(self.h[key])+<span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>*和/代表对应矩阵元素的运算</p>
</blockquote>
<p>adagrad的优点是对于梯度很大的噪音（例如震荡梯度）可以很好地去除，但是如果损失函数噪音很小，会导致学习率迅速下降，以至于无法更新。</p>
<h2 id="其他">其他</h2>
<p>基于adafrad和momentum还有RMSprop和adam算法等，本书只是给出了代码但没有解释。简单来说，RMSprop能够在一定程度上解决adagrad学习率迅速下降的问题，adam则融合了adafrad和momentum。</p>
<p><strong>参数的更新方法没有elixir，每种方法都有适用条件，例如许多人还在使用SGD</strong></p>
<h1>权重初始化</h1>
<p>之前的学习中，权重都是使用np.random.randn获得的，其含义是建立符合高斯分布的随机参数矩阵。值得注意的是，一般还会乘以一个系数（例如1或0.1），这个系数对权重的初始化意义重大。如果系数过大（如1），输入信号在经过激活层后的信号分布集中于边界处。以sigmoid函数为例，输出信号集中于0和1附近。由于sigmoid函数在0、1附近的梯度接近于0，会产生梯度消失现象。如果系数过小（0.01以下），输出信号会集中于0.5附近。这样虽然避免了梯度消失，但是单一化的权重限制了多神经元的表现力。如果权重为零矩阵，第二层神经元的输入值相同（？？？），则反向传播时全部权重会进行相同的更新，神经网络失效。</p>
<blockquote>
<p>这里不太理解，如果偏置是随机值，则第二层输入值不应该不等吗？</p>
</blockquote>
<p>解决方法是：若激活函数为线性函数（左右对称且中央附近可视为线性函数，如sigmoid和tanh），使用标准差为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mroot><mfrac><mn>1</mn><mi>n</mi></mfrac><mrow></mrow></mroot></mrow><annotation encoding="application/x-tex">\sqrt[]{\frac{1}{n} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.84em;vertical-align:-0.6049em;"></span><span class="mord sqrt"><span class="root"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.3781em;"><span style="top:-2.3781em;"><span class="pstrut" style="height:2em;"></span><span class="sizing reset-size6 size1 mtight"><span class="mord mtight"></span></span></span></span></span></span></span><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2351em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.1951em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.88em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.88em' viewBox='0 0 400000 1944' preserveAspectRatio='xMinYMin slice'><path d='M983 90
l0 -0
c4,-6.7,10,-10,18,-10 H400000v40
H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
c53.7,-170.3,84.5,-266.8,92.5,-289.5z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6049em;"><span></span></span></span></span></span></span></span></span> 的高斯分布初始化；对于ReLU函数，使用标准差为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mroot><mfrac><mn>2</mn><mi>n</mi></mfrac><mrow></mrow></mroot></mrow><annotation encoding="application/x-tex">\sqrt[]{\frac{2}{n} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.84em;vertical-align:-0.6049em;"></span><span class="mord sqrt"><span class="root"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.3781em;"><span style="top:-2.3781em;"><span class="pstrut" style="height:2em;"></span><span class="sizing reset-size6 size1 mtight"><span class="mord mtight"></span></span></span></span></span></span></span><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2351em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.1951em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.88em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.88em' viewBox='0 0 400000 1944' preserveAspectRatio='xMinYMin slice'><path d='M983 90
l0 -0
c4,-6.7,10,-10,18,-10 H400000v40
H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
c53.7,-170.3,84.5,-266.8,92.5,-289.5z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6049em;"><span></span></span></span></span></span></span></span></span> 的高斯分布初始化。这里n是上一层的节点数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weight1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>) * np.sqrt(<span class="number">1.0</span>/n)</span><br><span class="line">weight2 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>) * np.sqrt(<span class="number">2.0</span>/n)</span><br></pre></td></tr></table></figure>
<h1>batch normalization</h1>
<p>权重初始化的目的是使激活值分布地更好。batch normalization是一种强制性让激活值分散的方法，即以mini batch为单位，使数据进行标准差为1、均值为0的正规化分布。batch normalization也可以layer化，位于affine层和激活层之间或激活层之后。</p>
<p><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/batch_normalization.png" alt=""></p>
<p>batch normalization的优势：</p>
<ol>
<li>提高学习速度</li>
<li>降低对权重初始值的依赖性</li>
<li>抑制过拟合（下节）</li>
</ol>
<h1>过拟合与正则化</h1>
<p>过拟合指模型只能拟合训练数据，而对其他的如测试数据拟合较差。深度学习追求的是泛化能力，因此抑制过拟合很重要。过拟合出现的原因有：</p>
<ol>
<li>参数多，表现能力强</li>
<li>训练数据少</li>
</ol>
<h2 id="权值衰减">权值衰减</h2>
<p>过拟合往往是因为权值过大导致的，权值衰减将损失函数加上 $ \frac{1}{2}\lambda W^2 $ （平方范数或L2范数），这样在反向传播时梯度会加上 $ \lambda W $ 。权值越大，减小的速度越快。 $ \lambda$ 为超参数权值衰减系数，权重矩阵的平方是所有元素的平方和，即：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>W</mi><mo>=</mo><msqrt><mrow><msubsup><mi>w</mi><mn>11</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>w</mi><mn>12</mn><mn>2</mn></msubsup><mo>+</mo><mo>…</mo></mrow></msqrt></mrow><annotation encoding="application/x-tex">W=\sqrt{w_{11}^2+w_{12}^2+\dots} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.84em;vertical-align:-0.5413em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2987em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959em;"><span style="top:-2.4337em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2663em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959em;"><span style="top:-2.4337em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2663em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="minner">…</span></span></span><span style="top:-3.2587em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.88em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.88em' viewBox='0 0 400000 1944' preserveAspectRatio='xMinYMin slice'><path d='M983 90
l0 -0
c4,-6.7,10,-10,18,-10 H400000v40
H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
c53.7,-170.3,84.5,-266.8,92.5,-289.5z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5413em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>权值衰减不能对付复杂的NN模型，需要使用下面的dropout</p>
<h2 id="dropout">dropout</h2>
<center><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/gif_test.gif" width = "" height = ""></center>
<p>dropout是一种随机删除神经元的方法，用来模拟多个神经网络的平均模型</p>
<h2 id="超参数优化">超参数优化</h2>
<p>神经网络的数据一般划分为training data、validation data和test data，其中验证数据validation data用于评估超参数的性能。经验性的方法是在超参数的某个范围内随机采样，根据accuracy评估。</p>
<h1>卷积神经网络</h1>
<p>卷积神经网络convolutional neural network，相对于之前的全连接网络只能输入一维数组，CNN能够直接反应多维数组的空间特征。CNN新增加了卷积层convolution layer和pooling layer，组成了卷积层-激活层-（池化层）的基本结构，取代了affine层-激活层。卷积层的输入和输出数据又可以称为input feature map和output feature map。很多神经网络在输出层前会会放置一个全连接层。</p>
<blockquote>
<p>全连接网络形容相邻层的所有神经元之间都有连接</p>
</blockquote>
<h2 id="卷积层">卷积层</h2>
<p>下次复习时再整吧</p>
<h2 id="池化层">池化层</h2>
<p>池化层通过将矩阵的每一个区域当成一个元素处理，<strong>缩小空间大小</strong>。如下图所示，处理区域为(2,2)，一般池化层的步幅为处理区域的窗口大小，这里为2。通过将处理区域的最大值元素提取出来组成新的元素，这样的处理称为max池化，适用于图像处理。此外还有average池化等。池化层没有学习参数，并且处理前后通道数不变。</p>
<p><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/pooling_layer.png" alt=""></p>
<blockquote>
<p>Sergey Ioffe and Christian Szegedy（2015）: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv:1502.03167 [cs] (February 2015).</p>
</blockquote>
<h2 id="卷积层和池化层的实现">卷积层和池化层的实现</h2>
<p>img2col函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.arange(<span class="number">32</span>).reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">im2col</span>(<span class="params">input_data, filter_h, filter_w, stride=<span class="number">1</span>, pad=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据</span></span><br><span class="line"><span class="string">    filter_h : 滤波器的高</span></span><br><span class="line"><span class="string">    filter_w : 滤波器的长</span></span><br><span class="line"><span class="string">    stride : 步幅</span></span><br><span class="line"><span class="string">    pad : 填充</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N, C, H, W = input_data.shape</span><br><span class="line">    out_h = (H + <span class="number">2</span>*pad - filter_h)//stride + <span class="number">1</span></span><br><span class="line">    out_w = (W + <span class="number">2</span>*pad - filter_w)//stride + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    img = np.pad(input_data, [(<span class="number">0</span>,<span class="number">0</span>), (<span class="number">0</span>,<span class="number">0</span>), (pad, pad), (pad, pad)], <span class="string">&#x27;constant&#x27;</span>)</span><br><span class="line">    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(filter_h):</span><br><span class="line">        y_max = y + stride*out_h</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(filter_w):</span><br><span class="line">            x_max = x + stride*out_w</span><br><span class="line">            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]</span><br><span class="line"></span><br><span class="line">    col = col.transpose(<span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).reshape(N*out_h*out_w, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> col</span><br><span class="line"></span><br><span class="line">b = im2col(a,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">b.ndim</span><br><span class="line">b.shape</span><br></pre></td></tr></table></figure>
<p>输入信号经过img2col处理后，转变为(N*out_h*out_w,通道数C*filter_h*filter_w)的矩阵。整个过程相当于将滤波器处理的顺序矩阵区域按行排列（下图）：左侧的矩阵为一个图片矩阵，每一个小三维矩阵代表滤波器要处理的区域，将其按照通道顺序排列，组成二维矩阵（右侧）</p>
<p><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/input_convolution_process.png" alt=""></p>
<blockquote>
<p>《深度学习入门——基于python的理论与实现》</p>
</blockquote>
<p>滤波器的处理与输入信号类似，将每一个滤波器矩阵根据通道数按列排放（这一点与输入信号不同，思考一下矩阵的乘法规律）<br>
<img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/convolution_layer.png" alt=""></p>
<blockquote>
<p>《深度学习入门——基于python的理论与实现》</p>
</blockquote>
<p><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/convolution_work.webp" alt=""></p>
<blockquote>
<p>能看懂就是胜利(bushi)</p>
</blockquote>
<p>池化层比卷积层简单不少，直接将处理区域按行排列即可；求取每一行的最大值，然后再reshape</p>
<p><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/pooling_process.png" alt=""></p>
<h2 id="卷积神经网络在干什么">卷积神经网络在干什么</h2>
<p>之前大三一门关于机器学习的课上第一次接触神经网络，当时稀里糊涂地用matlab搞了一下，一直好奇卷积神经网络究竟在干什么。这本书里终于给了我一点定性的认识。每一个卷积层都能识别出图像的特征，例如第一层响应边缘，第五层响应物体，最后一层（affine层）响应物体类别。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://liubaichen.github.io">Liu baichen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://liubaichen.github.io/posts/35526/">https://liubaichen.github.io/posts/35526/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://liubaichen.github.io" target="_blank">Welcome to kotobanoniwa</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp12.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/14982/" title="numpy"><img class="cover" src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp6.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">numpy</div></div></a></div><div class="next-post pull-right"><a href="/posts/11347/" title="在市场经济和计划经济的十字路口——新经济政策研究（1921-1929）"><img class="cover" src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp9.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">在市场经济和计划经济的十字路口——新经济政策研究（1921-1929）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/24073/" title="深度学习(1)——perceptron"><img class="cover" src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp2.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-28</div><div class="title">深度学习(1)——perceptron</div></div></a></div><div><a href="/posts/74e95c64/" title="线性代数基础"><img class="cover" src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp1.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-09</div><div class="title">线性代数基础</div></div></a></div><div><a href="/posts/ba53d637/" title="参数与数据处理"><img class="cover" src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp7.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-17</div><div class="title">参数与数据处理</div></div></a></div><div><a href="/posts/92fd094d/" title="pytorch解决简单的分类问题"><img class="cover" src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp14.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-16</div><div class="title">pytorch解决简单的分类问题</div></div></a></div><div><a href="/posts/8acc63f2/" title="自然语言处理——推理方法"><img class="cover" src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp14.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-08</div><div class="title">自然语言处理——推理方法</div></div></a></div><div><a href="/posts/6edc6e28/" title="自然语言处理——计数方法"><img class="cover" src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp1.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-06</div><div class="title">自然语言处理——计数方法</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/face.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Liu baichen</div><div class="author-info__description">摸鱼中...</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/liubaichen/liubaichen.github.io"><i class="fab fa-github"></i><span>github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/liubaichen/liubaichen.github.io" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#loss-function"><span class="toc-number">2.1.</span> <span class="toc-text">loss function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E6%AD%A5%E9%AA%A4"><span class="toc-number">2.2.</span> <span class="toc-text">学习步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">随机梯度下降法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">误差反向传播法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="toc-number">4.1.</span> <span class="toc-text">理论基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%82%E6%9E%84%E5%BB%BA"><span class="toc-number">4.2.</span> <span class="toc-text">层构建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA"><span class="toc-number">4.3.</span> <span class="toc-text">神经网络构建</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">参数更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD"><span class="toc-number">5.1.</span> <span class="toc-text">SGD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#momentum"><span class="toc-number">5.2.</span> <span class="toc-text">momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#adagrad"><span class="toc-number">5.3.</span> <span class="toc-text">adagrad</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">5.4.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">权重初始化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">batch normalization</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">过拟合与正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E5%80%BC%E8%A1%B0%E5%87%8F"><span class="toc-number">8.1.</span> <span class="toc-text">权值衰减</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dropout"><span class="toc-number">8.2.</span> <span class="toc-text">dropout</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-number">8.3.</span> <span class="toc-text">超参数优化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">9.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">9.1.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">9.2.</span> <span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%92%8C%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">9.3.</span> <span class="toc-text">卷积层和池化层的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9C%A8%E5%B9%B2%E4%BB%80%E4%B9%88"><span class="toc-number">9.4.</span> <span class="toc-text">卷积神经网络在干什么</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/6a4b628f/" title="战后安排与冷战起源"><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp8.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="战后安排与冷战起源"/></a><div class="content"><a class="title" href="/posts/6a4b628f/" title="战后安排与冷战起源">战后安排与冷战起源</a><time datetime="2023-04-19T13:41:07.000Z" title="发表于 2023-04-19 21:41:07">2023-04-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/ba53d637/" title="参数与数据处理"><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp7.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="参数与数据处理"/></a><div class="content"><a class="title" href="/posts/ba53d637/" title="参数与数据处理">参数与数据处理</a><time datetime="2023-04-17T07:33:28.000Z" title="发表于 2023-04-17 15:33:28">2023-04-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/8d88f5e2/" title="pytorch解决回归问题——以高次函数为例"><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp6.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pytorch解决回归问题——以高次函数为例"/></a><div class="content"><a class="title" href="/posts/8d88f5e2/" title="pytorch解决回归问题——以高次函数为例">pytorch解决回归问题——以高次函数为例</a><time datetime="2023-04-16T12:38:04.000Z" title="发表于 2023-04-16 20:38:04">2023-04-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/92fd094d/" title="pytorch解决简单的分类问题"><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp14.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pytorch解决简单的分类问题"/></a><div class="content"><a class="title" href="/posts/92fd094d/" title="pytorch解决简单的分类问题">pytorch解决简单的分类问题</a><time datetime="2023-04-16T08:55:59.000Z" title="发表于 2023-04-16 16:55:59">2023-04-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/d14bfa85/" title="神经网络学习笔记"><img src="https://picgo-liubaichen.oss-cn-beijing.aliyuncs.com/bp10.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="神经网络学习笔记"/></a><div class="content"><a class="title" href="/posts/d14bfa85/" title="神经网络学习笔记">神经网络学习笔记</a><time datetime="2023-04-16T02:30:32.000Z" title="发表于 2023-04-16 10:30:32">2023-04-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By Liu baichen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to kotobanoniwa!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'gYWXNQxtQjnCNqN12eXIYtpl-gzGzoHsz',
      appKey: 'kyVbdtKxRyyrjqAcGBai3xDp',
      avatar: 'robohash',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><div class="aplayer no-destroy" data-id="8839201266" data-server="tencent" data-type="playlist" data-fixed="true" data-mini="true" data-autoplay="false" data-theme="#00477d" data-lrctype=0 data-order="random" data-volume=0.7 > </div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: true,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', 'G-F8JFWLJ64K', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/penchan.model.json"},"display":{"position":"right","width":200,"height":400,"hOffset":0,"vOffset":-60},"mobile":{"show":true},"react":{"opacity":0.75},"log":false});</script></body></html>